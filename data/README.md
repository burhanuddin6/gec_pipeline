# Data Directory

This directory contains the following files and folders:

## Files
- `annotations.json`: The annotations generated by running the `error_annotation/annotation.py` script. 
- `annotations_with_ids.json`: The annotations with unique IDs. This file is generated by running the `error_annotation/assign_error_ids.py` script. which takes `annotations.json` as input.
- `annotations_dict.json`: Contains the annotations with percentage calculated. This file is generated by running the `error_annotation/assign_error_ids.py` script. which takes `annotations_with_ids.json` as input. 
- `lemma_word_dict.json`: Contains the mapping of lemmas to words. This file is generated by a 500k sentences dataset from internet. The source can be shared upon request.
- `urdu_word_dict1.json`: Contains the mapping of Urdu words to their lemmas. This file is generated by running the `data_generation/generate_word_dict.py` script.
- `urdu_word_dict2.json`: Its a subset of `urdu_word_dict1.json` containing only the words that are present in the Urdu Dependency Treebank. This file is generated by running the `data_generation/generate_word_dict.py` script. We use this file instead of `urdu_word_dict1.json`
 in our code.
- `urdu_words.json`: Contains the list of Urdu words obtained from github.com/urduhack/urduwords.
- `raw_wikiedits.txt`: Contains the raw Wikipedia edits dataset of approximately 220k sentences.

## Folders
- `cleaned_correct_corpus/`: Contains the cleaned correct corpus. This corpus is used to generate negative samples. 
- `conllu/`: Contains the CoNLL-U files from Urdu Dependency Treebank.
- `out/`: This folder contains the output of the data infliction process.
- `wikiedits/`: This folder contains the Wikipedia edits dataset after preprocessing. To check what file was generated at each stage, refer to the `data_generation/clean.py` script.